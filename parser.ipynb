{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url to first pages\n",
    "# enter cian.ru and avito.ru urls of desired city and a name of the city\n",
    "cian_url = \"https://spb.cian.ru/cat.php?deal_type=sale&engine_version=2&offer_type=flat&region=2&room1=1&room2=1&room3=1&room4=1&room5=1&room6=1&room7=1&room9=1\"\n",
    "# avito_url = \"https://www.avito.ru/sankt-peterburg/kvartiry?s_trg=1\"\n",
    "avito_url = 'https://www.avito.ru/sankt-peterburg/kvartiry/prodam?f=549_5695-5696-5697-5698-5699-5700-5701-11018-11019-11020-11021&s_trg=4'\n",
    "city = \"Санкт-Петербург\"\n",
    "\n",
    "#example of samara urls\n",
    "# avito_url = 'https://www.avito.ru/samara/kvartiry?s_trg=3'\n",
    "# cian_url = 'https://samara.cian.ru/cat.php?deal_type=sale&engine_version=2&offer_type=flat&region=4966&room1=1&room2=1&room3=1&room4=1&room5=1&room6=1&room9=1'\n",
    "# city = 'Самара'\n",
    "\n",
    "api_key = '800e1c1a-3d68-496e-88a9-009b98d7eb75'\n",
    "api_keys = ['800e1c1a-3d68-496e-88a9-009b98d7eb75', 'c7bc2c4c-0414-4156-8bb3-bc12e8b38baa', 'c1ddf482-774f-4196-b40f-e1fcca928f34', '0b2344b3-fca3-4d27-a13e-5de997a53a4a']\n",
    "api_url = 'https://geocode-maps.yandex.ru/1.x/?apikey=' + api_key + '&format=json&geocode=' + city + ',+'\n",
    "api_urls = [f'https://geocode-maps.yandex.ru/1.x/?apikey={api_key}&format=json&geocode=' + city + ',+' for api_key in api_keys]\n",
    "tagged = {}\n",
    "keys_number = len(api_urls)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute this cell to load all neccessary functions\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import requests\n",
    "import re\n",
    "\n",
    "#write collected data to .csv file as a pandas dataframe\n",
    "import numpy\n",
    "import pandas as pd\n",
    "\n",
    "#decode geotagging answer\n",
    "import json\n",
    "import csv\n",
    "\n",
    "import time\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import sys\n",
    "\n",
    "def write_to_csv_tagged(new_data_flats, file_name):\n",
    "    a = numpy.asarray(new_data_flats)\n",
    "    pd.DataFrame(a).to_csv(file_name, header=['price', 'rooms', 'area', 'floor', 'floors_total', 'adress',\n",
    "                                              'subway_station', 'distance to subway', 'longitude', 'latitude'])\n",
    "    \n",
    "def geotag_single_row(row):\n",
    "    ind = row[0]\n",
    "    row = row[1]\n",
    "    print('requested address = ', row[6].replace(' ', '+'))\n",
    "    print('full url = ', api_urls[ind % keys_number] + row[6].replace(' ', '+'), 'ind = ', ind)\n",
    "    r = requests.get(api_urls[ind % keys_number] + row[6].replace(' ', '+'))\n",
    "    d = json.loads(r.text)\n",
    "    if d['response']['GeoObjectCollection']['metaDataProperty']['GeocoderResponseMetaData']['found'] != 0:\n",
    "        longitude, latitude = \\\n",
    "        d['response']['GeoObjectCollection']['featureMember'][0]['GeoObject']['Point']['pos'].split()\n",
    "        print('long = ', longitude, ', latitude = ', latitude)\n",
    "        tagged[ind] = (*row, longitude, latitude)\n",
    "    else:\n",
    "        print(\"geoposition not found, row skipped\")\n",
    "        \n",
    "def geotag_single_row_lat_long(row):\n",
    "    ind = row[0]\n",
    "    row = row[1]\n",
    "    print('requested address = ', row[0].replace(' ', '+'))\n",
    "    print('full url = ', api_urls[ind % keys_number] + row[0].replace(' ', '+'), 'ind = ', ind)\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(api_urls[ind % keys_number] + row[0].replace(' ', '+'))\n",
    "        d = json.loads(r.text)\n",
    "        if d['response']['GeoObjectCollection']['metaDataProperty']['GeocoderResponseMetaData']['found'] != 0:\n",
    "            longitude, latitude = \\\n",
    "            d['response']['GeoObjectCollection']['featureMember'][0]['GeoObject']['Point']['pos'].split()\n",
    "            print('long = ', longitude, ', latitude = ', latitude)\n",
    "            tagged[ind] = (*row, latitude, longitude)\n",
    "        else:\n",
    "            print(\"geoposition not found, row skipped\")\n",
    "    except Exception as error:\n",
    "        print(error, ' error, skip row')\n",
    "\n",
    "        \n",
    "def geotag_domofond(file_in, file_out):\n",
    "    \n",
    "    with open(file_in) as csv_file, open(file_out, 'w') as out:\n",
    "\n",
    "        writer = csv.writer(out, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        rows = [(ind,row) for ind, row in enumerate(csv_reader)]\n",
    "        \n",
    "        pool = ThreadPool()\n",
    "        results = pool.map(geotag_single_row_lat_long, rows)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        \n",
    "        t_keys = [key for key in tagged.keys()]\n",
    "        rows = [(tagged[key][0],tagged[key][1],tagged[key][2],tagged[key][3],tagged[key][4],tagged[key][5],tagged[key][6],tagged[key][7],tagged[key][8]) for key in t_keys ]\n",
    "        for row in rows:\n",
    "            writer.writerow(row)\n",
    "        \n",
    "#add geotagging to existing csv file\n",
    "def geotag(file_in, file_out):\n",
    "    \n",
    "    with open(file_in) as csv_file, open(file_out, 'w') as out:\n",
    "\n",
    "        writer = csv.writer(out, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        rows = [(ind,row) for ind, row in enumerate(csv_reader)]\n",
    "        \n",
    "        pool = ThreadPool()\n",
    "        results = pool.map(geotag_single_row, rows)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        \n",
    "        t_keys = [key for key in tagged.keys()]\n",
    "        rows = [(tagged[key][1],tagged[key][2],tagged[key][3],tagged[key][4],tagged[key][5],tagged[key][6],tagged[key][7],tagged[key][8],tagged[key][9],tagged[key][10]) for key in t_keys ]\n",
    "        for row in rows:\n",
    "            writer.writerow(row)\n",
    "\n",
    "#avito\n",
    "#extract price\n",
    "def price_from_json(flat):\n",
    "    price = flat.find(\"div\", {\"class\": \"popup-prices\"})\n",
    "    if price != None:\n",
    "        price = price['data-prices']\n",
    "    else:\n",
    "        return -1\n",
    "    price_ind = price.find('RUB\":')\n",
    "    price_cut = price[price_ind + 5:]\n",
    "    price_in_rub = price_cut.split(\",\")[0]\n",
    "    return int(price_in_rub)\n",
    "\n",
    "\n",
    "\n",
    "#extract all relevant data from single page\n",
    "def parse_page_avito(flats, data_flats):\n",
    "    rent_count = 0\n",
    "    print(\"flats on page: \", len(flats))\n",
    "    for (index, flat) in enumerate(flats):\n",
    "\n",
    "        print(\"Flat number\", index)\n",
    "        #price\n",
    "        print(\"price:\")\n",
    "        price = price_from_json(flat)\n",
    "        if (price < 300_000):\n",
    "            rent_count += 1\n",
    "            print(\"Rent found, skip. price = \", price)\n",
    "            continue\n",
    "        print(price)\n",
    "        print()\n",
    "\n",
    "        #rooms, area, floor level\n",
    "        print(\"rooms:\")\n",
    "        description = flat.find(\"a\", {\"class\": \"item-description-title-link\"})['title']\n",
    "        try_rooms = description.split(\",\")[0][0]\n",
    "        if try_rooms.isnumeric():\n",
    "            rooms = int(try_rooms)\n",
    "        else:\n",
    "            rooms = 1\n",
    "        print(rooms)\n",
    "        print()\n",
    "\n",
    "        print(\"area:\")\n",
    "        area = float(description.split(\",\")[1].split(\" \")[1])\n",
    "        print(area, \"m^2\")\n",
    "        print()\n",
    "\n",
    "        print(description)\n",
    "\n",
    "        print(\"floor:\")\n",
    "        floor = description.split(\",\")[2].split(\" \")[1].split(\"/\")[0]\n",
    "        print('floors total = ', description)\n",
    "        floors_total = description.split(\",\")[2].split(\" \")[1].split(\"/\")[1]\n",
    "        print(\"\" + floor + \", total floors: \" + floors_total)\n",
    "        print()\n",
    "\n",
    "        #subway, distance to subway, adress\n",
    "        full_adress = flat.find(\"p\", {\"class\": \"address\"}).getText()\n",
    "\n",
    "        adress = ','.join(full_adress.split(\",\")[1:])\n",
    "        print(\"adress:\")\n",
    "        print(adress)\n",
    "        print()\n",
    "\n",
    "        subway = full_adress.split(\",\")[0]\n",
    "        for (ind, el) in enumerate(subway.split()):\n",
    "            try:\n",
    "                float(el)\n",
    "                subway_station = ' '.join(subway.split()[:ind])\n",
    "                distance = float(subway.split()[ind])\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        print(\"subway:\")\n",
    "        print(subway_station)\n",
    "        print()\n",
    "\n",
    "        if distance < 99:\n",
    "            distance *= 1000\n",
    "        print(\"distance to subway:\")\n",
    "        print(distance, \"meters\")\n",
    "        print()\n",
    "\n",
    "        print(\"go to next page\")\n",
    "        data_flats.append((price, rooms, area, floor, floors_total, adress, subway_station, distance))\n",
    "\n",
    "    print(rent_count)\n",
    "    return data_flats\n",
    "\n",
    "\n",
    "def parse_avito(avito_url):\n",
    "\n",
    "    data_flats = []\n",
    "    next_page = avito_url\n",
    "    print('about to open url: ', avito_url)\n",
    "    page_count = 0\n",
    "    url_base = 'https://www.avito.ru'\n",
    "    while True:\n",
    "        try:\n",
    "            page_count += 1\n",
    "            print(\"Page: \", page_count, 'url = ', next_page)\n",
    "            \n",
    "            # get next page\n",
    "            avito_real_estate = urllib.request.urlopen(next_page).read()\n",
    "            # parse avito with beautiful soup\n",
    "            avito_soup = BeautifulSoup(avito_real_estate)\n",
    "            # find html table with flat data\n",
    "            flats = avito_soup.findAll(\"div\", {\"class\": \"item_table-description\"})\n",
    "            # parse flats\n",
    "            data_flats = parse_page_avito(flats, data_flats)\n",
    "            print('current data len = ', len(data_flats))\n",
    "            #find next page\n",
    "            next_page = url_base  + avito_soup.find(\"a\", {\"class\": \"pagination-page\"}).get('href')\n",
    "\n",
    "            if page_count >= 100:\n",
    "                print('page count = 100, break')\n",
    "                break\n",
    "        except Exception as error:\n",
    "            print(error, ' error, break')\n",
    "            break\n",
    "\n",
    "    print(\"finish, page count = \", page_count)\n",
    "    print(\"data len: \", len(data_flats))\n",
    "    new_data_flats = []\n",
    "    for flat in data_flats:\n",
    "        new_flat = (flat[0], flat[1], flat[2], flat[3], flat[4], flat[5].replace('\\xa0', '', 2), flat[6], flat[7])\n",
    "        new_data_flats.append(new_flat)\n",
    "\n",
    "    return new_data_flats\n",
    "\n",
    "\n",
    "flat_map = {}\n",
    "def avito_thread(url):\n",
    "    try:\n",
    "        flats = parse_avito(url)\n",
    "    except:\n",
    "        print('avito thread exception with url: ', url)\n",
    "        return\n",
    "        \n",
    "    flat_map[url] = flats\n",
    "    print('finished parsing url = ', url, ', len = ', len(flats))\n",
    "\n",
    "def write_to_csv(new_data_flats, file_name):\n",
    "    a = numpy.asarray(new_data_flats)\n",
    "    pd.DataFrame(a).to_csv(file_name, header=['price', 'rooms', 'area', 'floor', 'floors_total', 'adress',\n",
    "                                              'subway_station', 'distance to subway'])\n",
    "    \n",
    "def avito_concurrent(out_file):\n",
    "    \n",
    "    rooms = [f'{i}-komnatnye' for i in range(1,4)]\n",
    "    price_grade = [0, 3_000_000, 6_000_000, 10_000_000, 20_000_000, 100_000_000]\n",
    "    price_ranges = [f'pmax={price_grade[i + 1]}&pmin={price_grade[i]}' for i in range(len(price_grade) - 1)]\n",
    "    urls = []\n",
    "    for room in rooms:\n",
    "        for price in price_ranges:\n",
    "            urls.append(f'https://www.avito.ru/sankt-peterburg/kvartiry/prodam/{room}?{price}&s_trg=4')\n",
    "    start = time.time()\n",
    "    pool = ThreadPool()\n",
    "    results = pool.map(avito_thread, urls)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    print('total finish, time = ', time.time() - start, ', data size = ', len(flat_map))\n",
    "    \n",
    "    return flat_map\n",
    "#     for key in flat_map.keys():\n",
    "#         flats.extend(flat_map[key])\n",
    "#     write_to_csv(flats, out_file)\n",
    "\n",
    "\n",
    "#cian\n",
    "\n",
    "def next_page_cian(first_page, soup):\n",
    "    next_p = soup.find(\"li\", {\"class\": re.compile('^.*list-item--active.*')})\n",
    "    if next_p and next_p.nextSibling:\n",
    "        return next_p.nextSibling.a['href']\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def parse_cian_page(cian_soup, data_flats):\n",
    "    counter = 0\n",
    "    flats = cian_soup.findAll(\"div\", {\"class\": \"c6e8ba5398--info-container--YhZ2y\"})\n",
    "    print('flats = ', len(flats))\n",
    "    for flat in flats:\n",
    "\n",
    "        counter += 1\n",
    "        # rooms, area, floor\n",
    "        addr = flat.find(\"div\", {\"class\": re.compile('^.*title.*')})\n",
    "        if addr:\n",
    "            try:\n",
    "                rooms = int(addr.text[0])\n",
    "                print('rooms = ', rooms)\n",
    "            except Exception:\n",
    "                rooms = 1\n",
    "            try:\n",
    "                area = float(addr.text.split(',')[1].split()[0])\n",
    "            except Exception as error:\n",
    "                print(error, ', area error, area = ', addr.text.split(',')[1].split()[0])\n",
    "                continue\n",
    "            print('full addr = ', addr.text)\n",
    "            print('area = ', area)\n",
    "            floor = int(addr.text.split(',')[2].split(' ')[1].split('/')[0])\n",
    "            print('floors = ', floor)\n",
    "            try:\n",
    "                floors_total = int(addr.text.split(',')[2].split(' ')[1].split('/')[1])\n",
    "            except Exception as error:\n",
    "                print('floors total error = ', error)\n",
    "            print('floors total = ', floors_total)\n",
    "\n",
    "            print(addr.text)\n",
    "\n",
    "        subway = flat.find(\"div\", {\"class\": re.compile('^.*underground-name.*')})\n",
    "        if subway:\n",
    "            subway = subway.text\n",
    "            print(subway)\n",
    "        else:\n",
    "            subway = None\n",
    "\n",
    "        remoteness = flat.find(\"div\", {\"class\": re.compile('^.*remoteness.*')})\n",
    "        if remoteness:\n",
    "            print(remoteness.text)\n",
    "            if remoteness.text.split('\\xa0')[2] == 'пешком':\n",
    "                remoteness = int(remoteness.text.split('\\xa0')[0]) * 100\n",
    "            else:\n",
    "                remoteness = int(remoteness.text.split('\\xa0')[0]) * 1000\n",
    "            print('distance to subway = ', remoteness)\n",
    "        else:\n",
    "            remoteness = None\n",
    "\n",
    "        adress = flat.findAll(\"a\", {\"class\": re.compile('^.*address.*')})\n",
    "        if adress:\n",
    "            if (len(adress) > 5):\n",
    "                adress = adress[4].text + ', ' + adress[5].text\n",
    "                print(adress)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        price = flat.find(\"div\", {\"class\": re.compile('^.*price.*')}).find(\"div\", {'class': re.compile('.*header.*')})\n",
    "        if price:\n",
    "            print(price.text)\n",
    "            price_array = price.text.split(' ')[:len(price.text.split(' ')) - 1]\n",
    "            price = 0\n",
    "            for el in price_array:\n",
    "                price *= 1000\n",
    "                price += int(el)\n",
    "            print('price = ', price)\n",
    "\n",
    "        data_flats.append((price, rooms, area, floor, floors_total, adress, subway, remoteness))\n",
    "        print()\n",
    "    return data_flats\n",
    "\n",
    "def parse_cian(cian_url):\n",
    "    base_url = cian_url.split('cian.ru')[0] + 'cian.ru'\n",
    "    print('cian url = ', cian_url)\n",
    "    cian_real_estate = urllib.request.urlopen(cian_url).read()\n",
    "    cian_soup = BeautifulSoup(cian_real_estate)\n",
    "    counter = 0\n",
    "    flats = []\n",
    "    pages = 58\n",
    "    while counter < pages:\n",
    "        counter += 1\n",
    "        time.sleep(30) \n",
    "        flats = parse_cian_page(cian_soup, flats)\n",
    "        cian_url = next_page_cian(cian_url, cian_soup)\n",
    "        if cian_url == -1:\n",
    "            break\n",
    "        if base_url not in cian_url:\n",
    "            cian_url = base_url + cian_url\n",
    "        print(\"url = \", cian_url)\n",
    "        cian_html = urllib.request.urlopen(cian_url).read()\n",
    "        cian_soup = BeautifulSoup(cian_html)\n",
    "        print(\"counter = \", counter)\n",
    "    return flats\n",
    "\n",
    "#merge two data files\n",
    "def merge(first, second, out):\n",
    "    with open(first) as cian, open(second) as avito, open(out, 'w') as out:\n",
    "        writer = csv.writer(out, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "        cian_reader = csv.reader(cian, delimiter=',')\n",
    "        avito_reader = csv.reader(avito, delimiter=',')\n",
    "\n",
    "        for row in avito_reader:\n",
    "            writer.writerow(row)\n",
    "\n",
    "        for row in cian_reader:\n",
    "            writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# необходимо сначала исполнить следующую ячейку, чтобы загрузить все функции\n",
    "# работает долго\n",
    "\n",
    "orig_stdout = sys.stdout\n",
    "with open('log.txt', 'w') as f:\n",
    "    sys.stdout = f\n",
    "\n",
    "    # avito to csv\n",
    "    print('start avito parsing, url = ', avito_url)\n",
    "    flat_map = avito_concurrent('spb_test_avito.csv')\n",
    "    print('finished avito parsing')\n",
    "    print('full data length = ', len(flat_map))\n",
    "    print('writing to ', out_file)\n",
    "    for key in flat_map.keys():\n",
    "        flats.extend(flat_map[key])\n",
    "    write_to_csv(flats, out_file)\n",
    "    print('writing finished')\n",
    "\n",
    "# # avito_flats = parse_avito(avito_url)\n",
    "# # write_to_csv(avito_flats, 'spb_test_avito.csv')\n",
    "\n",
    "\n",
    "# # cian to csv\n",
    "# print('start cian parsing, url = ', cian_url)\n",
    "# cian_flats = parse_cian(cian_url)\n",
    "# write_to_csv(cian_flats, 'samara_test_cian.csv')\n",
    "# print('finished cian parsing')\n",
    "\n",
    "# # merge\n",
    "# print('merging cian and avito data')\n",
    "# merge('samara_test_avito.csv', 'samara_test_cian.csv', 'samara_test_merged.csv')\n",
    "\n",
    "# geotag\n",
    "# print('geotag all data')\n",
    "# geotag_domofond('smr-domofond.csv', 'smr_geotagged.csv')\n",
    "# print(len(flats))\n",
    "# print(len(flats))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
